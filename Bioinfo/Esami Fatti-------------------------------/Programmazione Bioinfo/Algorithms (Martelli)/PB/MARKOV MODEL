MARKOV MODEL(p(SEQUENCE|MODEL))
begin=>state=>end
-eache state has total 1 probability split on all the possible transition to other state.
-the order(MEMORY) reppresent how mwany element before n to calculate the probability of n.
-the total probability is the sum of the probability of all the tranasition

TRAINING
-max likelyhood of obtaining the training set(only positive training set)
	parameters= max P(data|model)
	p(d|m)= sum all possible transition^n AND sum of each state = 1
	-use langrangian multipliers with N constrain = number of state
	 => P(transition)= specific transition(one element)\ total transition(same element)
-max a posteriori extimation [more correct but cant be computed]
	P(D|M, o)*P(o)
	=> we cant have P(o) a priori


HIDDEN MARKOV MODEL
markov model(transition) but each state can generate something(emission) elese based on a probability, usually the markov model path(Py(i)) is hidden and the sequence(Si) is visible.
=> starting from an osservation we can reconstruct the markov model under it.

N.B: if we have both emission and path the toatal probabiliti is the mutiplication of all the probability.

PROBLEMS
-P(sequenc|model) if path is hidden: sum of all specific emission probability over all the possible path(we know the markov model parameters but not the path).

DEFINITION:(n:state M:emission)
n+n^2+n total transition
n*m total emission probability

sum of total probability for one state =1
sum of total emission for one state =1

P ( Y ) = ∑ X P ( Y ∣ X ) P ( X )     x:state Y:outcome

---FORWARD ALGORITHM-------------------------------------------------------------------
	Fk(state)i(position)= find probability of specific sequence ending on a specific state 
-find the hidden path(!Viterbi path!)= max P(path|sequence)= max P(p,s|model)
-how to train(two cases: known path or not)

COMPILE
Matrix: possible state+ start, end X sequence+2 empty (start,end)
P next state(starting from start = 1)= p(prewious event)*P(all transitin to new state)*P(specific emission)

---BACKWARD ALGORITHM------------------------------------------------------------------
P(state | prewious state)= 

COMPILE
P prewious state= B k (T)· a B k · e k (s T )

---VITERBI PATH = max P(path|sequence)-------------------------------------------------
Compute the path given the sequence => VITERBY DECODING: given sequence compute the path

A POSTERIORI DECODING
position by position wich is the state that maximize the path

P(py(i)=k | s,M) = P(s,pi(i)=k|M) \ P(s|M) = Forward(i)*beckward(i) \ P(s|M)



TRAINING THE MODEL(maximum likelyhood)
EASY: we need to know both the sequence and the path and we can simply compute the statistical occurence.
HARD: we do NOT know the path P(py | s,teta0)                        teta0 = specific parameters of that model randomly given

---BAUM WELCH ALGORITHM
A k,l = ∑(over py) P( py | s, teta0 ) · A k,l ( py )
Considering that A k,l must be normalized to obtein a total chance of 1
For bot A k,l[transition from k to l] and E k(c)[emission of c from state k]


=> if we use this to generate teta1 it is better than teta0


APPLICATION
Using random parameters generate beckward and forward matrix and use them to compute the new parameters using baum welch and repeat unitil no farther improvment.




HHM IN BIOINFORMATICS (profile HHM [used by Pfam])
N.B: PSIBLAST use profiles that are markov model!

GAPS
We can model gaps like a transition to the next state skipping one(too long) or using state that has no emission probability but just probability to go to the next state ore to the next non generating state(silent state)

INSERTION
we need a third layer of states that can generate extra character in a given position and can only go to the next state, silent state or repeat himselfs.

USES:
-alignment
-P. familiy 
-secondary structure prediction: we need to introduce a !!!'grammar'!!! to have contraint on the state changing (labelled HHM)




